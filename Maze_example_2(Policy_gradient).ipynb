{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 시각적 미로 구현\n",
    "matplot을 통해 미로를 만들어 줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(5, 5))\n",
    "ax = plt.gca()\n",
    "\n",
    "plt.plot([1, 1], [0, 1], color=\"red\", linewidth=2)\n",
    "plt.plot([1, 2], [2, 2], color=\"red\", linewidth=2)\n",
    "plt.plot([2, 2], [2, 1], color=\"red\", linewidth=2)\n",
    "plt.plot([2, 3], [1, 1], color=\"red\", linewidth=2)\n",
    "\n",
    "plt.text(0.5, 2.5, \"S0\", size=14, ha=\"center\")\n",
    "plt.text(1.5, 2.5, \"S1\", size=14, ha=\"center\")\n",
    "plt.text(2.5, 2.5, \"S2\", size=14, ha=\"center\")\n",
    "plt.text(0.5, 1.5, \"S3\", size=14, ha=\"center\")\n",
    "plt.text(1.5, 1.5, \"S4\", size=14, ha=\"center\")\n",
    "plt.text(2.5, 1.5, \"S5\", size=14, ha=\"center\")\n",
    "plt.text(0.5, 0.5, \"S6\", size=14, ha=\"center\")\n",
    "plt.text(1.5, 0.5, \"S7\", size=14, ha=\"center\")\n",
    "plt.text(2.5, 0.5, \"S8\", size=14, ha=\"center\")\n",
    "plt.text(0.5, 2.3, \"START\", ha=\"center\")\n",
    "plt.text(2.5, 0.3, \"GOAL\", ha=\"center\")\n",
    "\n",
    "ax.set_xlim(0, 3)\n",
    "ax.set_ylim(0, 3)\n",
    "plt.tick_params(axis='both', which='both', bottom=False, top=False, labelbottom=False, right=False, left=False, labelleft=False)\n",
    "\n",
    "line, = ax.plot([0.5], [2.5], marker=\"o\", color=\"g\", markersize=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 정책 반복 알고리즘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 정책의 초기값 설정\n",
    "에이전트의 정책을 결정하는 파라미터의 초기값인 theta_0을 설정해줍니다.  \n",
    "각 줄은 state 0~7 각각 취할 수 있는 행동들을 의미합니다.  \n",
    "행동은 각각 상, 우, 하, 좌에 해당됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_0 = np.array([[np.nan, 1, 1, np.nan],      # s0\n",
    "                    [np.nan, 1, np.nan, 1],      # s1\n",
    "                    [np.nan, np.nan, 1, 1],      # s2\n",
    "                    [1, 1, 1, np.nan],           # s3\n",
    "                    [np.nan, np.nan, 1, 1],      # s4\n",
    "                    [1, np.nan, np.nan, np.nan], # s5\n",
    "                    [1, np.nan, np.nan, np.nan], # s6\n",
    "                    [1, 1, np.nan, np.nan],      # s7\n",
    "                    # s8은 목표 지점이기에 정책 x\n",
    "                   ])\n",
    "\n",
    "np.set_printoptions(precision=3, suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 초기 정책 정의\n",
    "파라미터 theta를 기반으로 정책을 계산합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 단순 비율 계산\n",
    "이 계산 과정은 각 행동이 똑같은 확률을 가지도록 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def simple_convert_into_pi_from_theta(theta):\n",
    "#     [m, n] = theta.shape\n",
    "#     pi = np.zeros((m, n))\n",
    "#     for i in range(0, m):\n",
    "#         pi[i, :] = theta[i, :] / np.nansum(theta[i, :])\n",
    "        \n",
    "#     pi = np.nan_to_num(pi)\n",
    "    \n",
    "#     return pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 소프트맥스 함수 계산\n",
    "변화하는 파라미터가 음수가 되는 경우에도 정책을 계산할 수 있도록 항상 양수의 값을 내놓는 지수 함수로 정책 함수를 만들어 줍니다.  \n",
    "$$ P(\\theta_i)=\\frac{exp(\\beta \\theta_i)}{exp(\\beta \\theta_1)+exp(\\beta \\theta_2)+\\cdots} $$  \n",
    "$$ = \\frac{exp(\\beta \\theta_i)}{\\sum_{j=1}^{N_a}exp(\\beta \\theta_j)} $$  \n",
    "$\\beta$는 행동이 무작위로 선택되는 정도를 의미한다. 작을수록 더욱 무작위로 선택된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_convert_into_pi_from_theta(theta):\n",
    "    beta = 1.0\n",
    "    [m, n] = theta.shape\n",
    "    pi = np.zeros((m, n))\n",
    "    \n",
    "    exp_theta = np.exp(beta * theta)\n",
    "    \n",
    "    for i in range(0, m):\n",
    "        pi[i, :] = exp_theta[i, :] / np.nansum(exp_theta[i, :])\n",
    "        \n",
    "    pi = np.nan_to_num(pi)\n",
    "    \n",
    "    return pi        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 초기 정책 가져오기\n",
    "정의한 함수를 통해 에이전트를 움직일 초기 정책을 구합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.   , 0.5  , 0.5  , 0.   ],\n",
       "       [0.   , 0.5  , 0.   , 0.5  ],\n",
       "       [0.   , 0.   , 0.5  , 0.5  ],\n",
       "       [0.333, 0.333, 0.333, 0.   ],\n",
       "       [0.   , 0.   , 0.5  , 0.5  ],\n",
       "       [1.   , 0.   , 0.   , 0.   ],\n",
       "       [1.   , 0.   , 0.   , 0.   ],\n",
       "       [0.5  , 0.5  , 0.   , 0.   ]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pi_0 = simple_convert_into_pi_from_theta(theta_0)\n",
    "pi_0 = softmax_convert_into_pi_from_theta(theta_0)\n",
    "\n",
    "pi_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 이동과 도착 함수 정의\n",
    "에이전트가 한 단계 이동하여 상태를 바꿀 수 있도록 함수를 만듭니다.  \n",
    "이 함수를 반복하여 도착 지점에 도달했는지 판단하는 함수도 만들어 줍니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_next_s(pi, s):\n",
    "#     direction = [\"up\", \"right\", \"down\", \"left\"]\n",
    "    \n",
    "#     next_direction = np.random.choice(direction, p=pi[s, :])\n",
    "    \n",
    "#     if next_direction == \"up\":\n",
    "#         s_next = s - 3\n",
    "#     elif next_direction == \"right\":\n",
    "#         s_next = s + 1\n",
    "#     elif next_direction == \"down\":\n",
    "#         s_next = s + 3\n",
    "#     elif next_direction == \"left\":\n",
    "#         s_next = s - 1\n",
    "        \n",
    "#     return s_next\n",
    "\n",
    "# def goal_maze(pi):\n",
    "#     s = 0\n",
    "#     state_history = [0]\n",
    "    \n",
    "#     while True:\n",
    "#         next_s = get_next_s(pi, s)\n",
    "#         state_history.append(next_s)\n",
    "        \n",
    "#         if next_s == 8:\n",
    "#             break\n",
    "#         else:\n",
    "#             s = next_s\n",
    "            \n",
    "#     return state_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action_and_next_s(pi, s):\n",
    "    direction = [\"up\", \"right\", \"down\", \"left\"]\n",
    "    \n",
    "    next_direction = np.random.choice(direction, p=pi[s, :])\n",
    "    \n",
    "    if next_direction == \"up\":\n",
    "        action = 0\n",
    "        s_next = s - 3\n",
    "    elif next_direction == \"right\":\n",
    "        action = 1\n",
    "        s_next = s + 1\n",
    "    elif next_direction == \"down\":\n",
    "        action = 2\n",
    "        s_next = s + 3\n",
    "    elif next_direction == \"left\":\n",
    "        action = 3\n",
    "        s_next = s - 1\n",
    "        \n",
    "    return [action, s_next]\n",
    "\n",
    "def goal_maze_ret_s_a(pi):\n",
    "    s = 0\n",
    "    s_a_history = [{'state':0, 'action':np.nan}]\n",
    "    \n",
    "    while (1):\n",
    "        [action, next_s] = get_action_and_next_s(pi, s)\n",
    "        s_a_history[-1]['action'] = action\n",
    "        \n",
    "        s_a_history.append({'state':next_s, 'action':np.nan})\n",
    "        \n",
    "        if next_s == 8:\n",
    "            break\n",
    "        else:\n",
    "            s = next_s\n",
    "            \n",
    "    return s_a_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 초기 정책의 결과\n",
    "초기 정책으로 에이전트를 움직였을 때, 상태와 행동의 기록을 출력한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1], [1, 3], [0, 1], [1, 3], [0, 1], [1, 3], [0, 2], [3, 2], [6, 0], [3, 0], [0, 2], [3, 1], [4, 3], [3, 2], [6, 0], [3, 1], [4, 3], [3, 0], [0, 1], [1, 1], [2, 2], [5, 0], [2, 2], [5, 0], [2, 3], [1, 3], [0, 1], [1, 1], [2, 2], [5, 0], [2, 2], [5, 0], [2, 3], [1, 1], [2, 3], [1, 1], [2, 3], [1, 1], [2, 3], [1, 3], [0, 2], [3, 1], [4, 2], [7, 1], [8, nan], \n",
      "목표 지점에 이르기까지 걸린 단계 수 : 44\n"
     ]
    }
   ],
   "source": [
    "s_a_history = goal_maze_ret_s_a(pi_0)\n",
    "\n",
    "for h in s_a_history:\n",
    "    print([h['state'], h['action']], end=', ')\n",
    "print()\n",
    "    \n",
    "print(\"목표 지점에 이르기까지 걸린 단계 수 :\", len(s_a_history)-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 정책 수정\n",
    "Policy Gradient 알고리즘으로 파라미터를 수정한다.  \n",
    "$$ \\theta_{s_i, a_j}=\\theta_{s_i, a_j}+\\eta \\cdot \\Delta \\theta_{s, a_j} $$  \n",
    "$$ \\Delta \\theta_{s, a_j} = \\{N(s_i, a_j)-P(s_i, a_j)N(s_i, a)\\}/T $$\n",
    "$\\eta$는 학습률로, 한 학습에 얼마나 수정되냐를 결정한다. 작을 수록 학습 속도가 늦어지고, 너무 높으면 오버피팅 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_theta(theta, pi, s_a_history):\n",
    "    eta = 0.1\n",
    "    T = len(s_a_history)-1\n",
    "    \n",
    "    [m, n] = theta.shape\n",
    "    delta_theta = theta.copy()\n",
    "    \n",
    "    for i in range(0, m):\n",
    "        for j in range(0, n):\n",
    "            if not np.isnan(theta[i, j]):\n",
    "                SA_i = [SA for SA in s_a_history if SA['state'] == i]\n",
    "                SA_ij = [SA for SA in SA_i if SA['action'] == j]\n",
    "                \n",
    "                if not SA_ij == [SA for SA in s_a_history if SA == {'state':i, 'action':j}]:\n",
    "                    raise Exception(\"not equal error\")\n",
    "                \n",
    "                N_i = len(SA_i)\n",
    "                N_ij = len(SA_ij)\n",
    "                \n",
    "                delta_theta[i, j] = (N_ij - pi[i, j] * N_i) / T\n",
    "            \n",
    "    new_theta = theta + eta*delta_theta\n",
    "    \n",
    "    return new_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 정책 반복 알고리즘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stop_epsilon = 10**-4\n",
    "\n",
    "theta = theta_0\n",
    "pi = pi_0\n",
    "\n",
    "is_continue = True\n",
    "count = 1\n",
    "while is_continue:\n",
    "    s_a_history = goal_maze_ret_s_a(pi)\n",
    "    new_theta = update_theta(theta, pi, s_a_history)\n",
    "    new_pi = softmax_convert_into_pi_from_theta(new_theta)\n",
    "    \n",
    "    print(np.sum(np.abs(new_pi - pi)))\n",
    "    print(\"목표 지점에 이르기까지 걸린 단계 수:\", len(s_a_history)-1)\n",
    "    \n",
    "    if np.sum(np.abs(new_pi - pi)) < stop_epsilon:\n",
    "        is_continue = False\n",
    "    else:\n",
    "        theta = new_theta\n",
    "        pi = new_pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 가치 반복 알고리즘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 가치 함수 무작위 정의\n",
    "가치 반복 알고리즘에서 사용할 가치 함수 Q를 정의해준다.  \n",
    "초기값은 무작위로 정해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[a, b] = theta_0.shape\n",
    "Q = np.random.rand(a, b) * theta_0 * 0.1\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 무작위 정책 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_0 = simple_convert_into_pi_from_theta(theta_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\epsilon$-greedy 알고리즘 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(s, Q, epsilon, pi_0):\n",
    "    direction = [\"up\", \"right\", \"down\", \"left\"]\n",
    "\n",
    "    if np.random.rand() < epsilon:\n",
    "        next_direction = np.random.choice(direction, p=pi_0[s, :])\n",
    "    else:\n",
    "        next_direction = direction[np.nanargmax(Q[s, :])]\n",
    "                \n",
    "    action = direction.index(next_direction)\n",
    "        \n",
    "    return action\n",
    "\n",
    "def get_s_next(s, a, Q, epsilon):\n",
    "    direction = [\"up\", \"right\", \"down\", \"left\"]\n",
    "    next_direction = direction[a]\n",
    "    \n",
    "    if next_direction == \"up\":\n",
    "        s_next = s - 3\n",
    "    elif next_direction == \"right\":\n",
    "        s_next = s + 1\n",
    "    elif next_direction == \"down\":\n",
    "        s_next = s + 3\n",
    "    elif next_direction == \"left\":\n",
    "        s_next = s - 1\n",
    "        \n",
    "    return s_next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sarsa 알고리즘 구현\n",
    "벨만 방정식에 기초하여 TD 오차를 바탕으로 행동가치 함수 Q를 수정하는 알고리즘을 만들어준다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sarsa(s, a, r, s_next, a_next, Q, eta, gamma):\n",
    "    if s_next == 8:\n",
    "        Q[s, a] = Q[s, a] + eta*(r-Q[s, a])\n",
    "    else:\n",
    "        Q[s, a] = Q[s, a] + eta*(r + gamma*Q[s_next, a_next] - Q[s, a])\n",
    "        \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-learning 알고리즘 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q_learning(s, a, r, s_next, a_next, Q, eta, gamma):\n",
    "    if s_next == 8:\n",
    "        Q[s, a] = Q[s, a] + eta*(r-Q[s, a])\n",
    "    else:\n",
    "        Q[s, a] = Q[s, a] + eta*(r + gamma*np.nanmax(Q[s_next, :]) - Q[s, a])\n",
    "        \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 도착 함수 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def goal_maze_ret_s_a_Q(Q, epsilon, eta, gamma, pi, update_func):\n",
    "    s = 0\n",
    "    a = a_next = get_action(s, Q, epsilon, pi)\n",
    "    s_a_history = [{'s':0, 'a':np.nan}]\n",
    "    \n",
    "    while True:\n",
    "        a = a_next\n",
    "        \n",
    "        s_a_history[-1]['a'] = a\n",
    "        \n",
    "        s_next = get_s_next(s, a, Q, epsilon)\n",
    "        \n",
    "        s_a_history.append({'s':s_next, 'a':np.nan})\n",
    "        \n",
    "        if s_next == 8:\n",
    "            r = 1\n",
    "            a_next = np.nan\n",
    "        else:\n",
    "            r = 0\n",
    "            a_next = get_action(s_next, Q, epsilon, pi)\n",
    "            \n",
    "        Q = update_func(s, a, r, s_next, a_next, Q, eta, gamma)\n",
    "        \n",
    "        if s_next == 8:\n",
    "            break\n",
    "        else:\n",
    "            s = s_next\n",
    "            \n",
    "    return [s_a_history, Q]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 가치 반복 알고리즘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.1\n",
    "gamma = 0.9\n",
    "epsilon = 0.5\n",
    "v = np.nanmax(Q, axis=1)\n",
    "is_continue = True\n",
    "episode = 1\n",
    "\n",
    "V = []\n",
    "V.append(v)\n",
    "\n",
    "while is_continue:\n",
    "    print(\"에피소드:\", episode)\n",
    "    \n",
    "    epsilon = epsilon / 2\n",
    "    \n",
    "#     [s_a_history, Q] = goal_maze_ret_s_a_Q(Q, epsilon, eta, gamma, pi_0, update_func=Sarsa)\n",
    "    [s_a_history, Q] = goal_maze_ret_s_a_Q(Q, epsilon, eta, gamma, pi_0, update_func=Q_learning)\n",
    "    \n",
    "    new_v = np.nanmax(Q, axis=1)\n",
    "    print(np.sum(np.abs(new_v - v)))\n",
    "    v = new_v\n",
    "    V.append(v)\n",
    "    \n",
    "    print(\"목표 지점에 이르기까지 걸린 단계 수:\", len(s_a_history)-1)\n",
    "    \n",
    "    episode = episode + 1\n",
    "    if episode > 100:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 애니메이션 시각화\n",
    "에이전트의 이동 과정을 애니메이션으로 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import animation\n",
    "from IPython.display import HTML\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "def init():\n",
    "    line.set_data([], [])\n",
    "    return (line,)\n",
    "\n",
    "def animate_history(i):\n",
    "    state = s_a_history[i]['s']\n",
    "    x = (state % 3) + 0.5\n",
    "    y = 2.5 - int(state / 3)\n",
    "    line.set_data(x, y)\n",
    "    return (line, )\n",
    "\n",
    "anim_history = animation.FuncAnimation(fig, animate_history, init_func=init, frames=len(s_a_history), interval=100, repeat=False)\n",
    "\n",
    "HTML(anim_history.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def animate_episode(i):\n",
    "    line, = ax.plot([0.5], [2.5], marker=\"s\", color=cm.jet(V[i][0]), markersize=85)\n",
    "    line, = ax.plot([1.5], [2.5], marker=\"s\", color=cm.jet(V[i][1]), markersize=85)\n",
    "    line, = ax.plot([2.5], [2.5], marker=\"s\", color=cm.jet(V[i][2]), markersize=85)\n",
    "    line, = ax.plot([0.5], [1.5], marker=\"s\", color=cm.jet(V[i][3]), markersize=85)\n",
    "    line, = ax.plot([1.5], [1.5], marker=\"s\", color=cm.jet(V[i][4]), markersize=85)\n",
    "    line, = ax.plot([2.5], [1.5], marker=\"s\", color=cm.jet(V[i][5]), markersize=85)\n",
    "    line, = ax.plot([0.5], [0.5], marker=\"s\", color=cm.jet(V[i][6]), markersize=85)\n",
    "    line, = ax.plot([1.5], [0.5], marker=\"s\", color=cm.jet(V[i][7]), markersize=85)\n",
    "    line, = ax.plot([2.5], [0.5], marker=\"s\", color=cm.jet(1.0), markersize=85)\n",
    "    return (line, )\n",
    "\n",
    "anim_episode = animation.FuncAnimation(fig, animate_episode, init_func=init, frames=len(V), interval=100, repeat=False)\n",
    "\n",
    "HTML(anim_episode.to_jshtml())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
